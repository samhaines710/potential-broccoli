name: Generate Data, Train & Publish Model

on:
  push:
    paths:
      - "Grond-main 2/**"
      - ".github/workflows/train_and_publish_model.yml"
  workflow_dispatch:

jobs:
  train-and-upload:
    name: Train & Upload to S3
    runs-on: ubuntu-latest
    environment: CI

    env:
      # Subfolder (quoted because of the space)
      APP_DIR: "Grond-main 2"

      # Region (set repo variable AWS_DEFAULT_REGION, e.g., eu-north-1)
      AWS_REGION: ${{ vars.AWS_DEFAULT_REGION }}
      AWS_DEFAULT_REGION: ${{ vars.AWS_DEFAULT_REGION }}

      # Canonical naming (bucket + model)
      S3_BUCKET: tinabobina
      MODEL_KEY: models/tinabobina.pipeline.joblib
      MODEL_OUT: models/tinabobina.pipeline.joblib

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Show repo layout (debug)
        run: |
          pwd
          ls -la
          ls -la "${{ env.APP_DIR }}" || true

      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: "${{ runner.os }}-pip-${{ hashFiles('Grond-main 2/requirements.txt') }}"
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        working-directory: "${{ env.APP_DIR }}"
        run: |
          python -m pip install --upgrade pip
          pip install --no-cache-dir -r requirements.txt boto3 awscli

      - name: Ensure local utils is a package
        working-directory: "${{ env.APP_DIR }}"
        run: |
          python - <<'PY'
import pathlib
p = pathlib.Path("utils/__init__.py")
p.parent.mkdir(parents=True, exist_ok=True)
if not p.exists():
    p.write_text("# package\n")
print("utils/ package present")
PY

      - name: Prepare training data
        working-directory: "${{ env.APP_DIR }}"
        env:
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
        run: |
          mkdir -p data
          python -u prepare_training_data.py
          test -s data/movement_training_data.csv

      - name: Validate AWS region variable
        shell: bash
        run: |
          if [[ -z "${AWS_REGION:-}" ]]; then
            echo "ERROR: Repository variable AWS_DEFAULT_REGION is not set." >&2
            exit 1
          fi
          echo "AWS region: ${AWS_REGION}"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Verify caller and region
        run: |
          aws sts get-caller-identity
          aws configure list

      # Resolve target S3 URIs. If ML_MODEL_PATH is set as a repo variable, use it;
      # otherwise default to s3://tinabobina/models/tinabobina.pipeline.joblib
      - name: Resolve S3 URIs (use ML_MODEL_PATH if provided)
        id: s3vars
        shell: bash
        env:
          ML_MODEL_PATH_VAR: ${{ vars.ML_MODEL_PATH }}
          S3_BUCKET: ${{ env.S3_BUCKET }}
          MODEL_KEY: ${{ env.MODEL_KEY }}
        run: |
          set -euo pipefail
          if [[ -n "${ML_MODEL_PATH_VAR:-}" ]]; then
            uri="${ML_MODEL_PATH_VAR}"
          else
            uri="s3://${S3_BUCKET}/${MODEL_KEY}"
            echo "ML_MODEL_PATH not set; defaulting to: ${uri}"
          fi
          if [[ "${uri}" != s3://* ]]; then
            echo "ERROR: ML_MODEL_PATH must be an s3:// URI (got: ${uri})" >&2
            exit 1
          fi
          bucket="${uri#s3://}"; bucket="${bucket%%/*}"
          echo "bucket=${bucket}" >> "$GITHUB_OUTPUT"
          echo "model_uri=${uri}" >> "$GITHUB_OUTPUT"
          echo "csv_uri=s3://${bucket}/data/movement_training_data.csv" >> "$GITHUB_OUTPUT"
          echo "Resolved:"
          echo "  model_uri: ${uri}"
          echo "  csv_uri:   s3://${bucket}/data/movement_training_data.csv"

      - name: Upload training CSV to S3
        working-directory: "${{ env.APP_DIR }}"
        run: |
          aws s3 cp data/movement_training_data.csv "${{ steps.s3vars.outputs.csv_uri }}" --only-show-errors

      - name: Train model pipeline (tinabobina)
        working-directory: "${{ env.APP_DIR }}"
        run: |
          mkdir -p models
          python -u train_ml_classifier.py \
            --train-csv data/movement_training_data.csv \
            --label-col movement_type \
            --model-dir models \
            --model-filename "$(basename "${MODEL_OUT}")"
          test -s "${MODEL_OUT}"

      - name: Upload model artifact to S3
        working-directory: "${{ env.APP_DIR }}"
        run: |
          aws s3 cp "${MODEL_OUT}" "${{ steps.s3vars.outputs.model_uri }}" --only-show-errors

      - name: Upload artifacts to GitHub
        uses: actions/upload-artifact@v4
        with:
          name: model-and-data
          path: |
            Grond-main 2/data/movement_training_data.csv
            Grond-main 2/${{ env.MODEL_OUT }}
          if-no-files-found: error
