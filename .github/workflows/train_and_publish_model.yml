name: Train & Publish HCBC Model

on:
  workflow_dispatch:
    inputs:
      n_trials:
        description: 'Optuna trials'
        required: false
        default: '40'
      folds:
        description: 'Walk-forward folds'
        required: false
        default: '6'
      embargo_rows:
        description: 'Embargo rows between train/val'
        required: false
        default: '200'

permissions:
  contents: write
  id-token: write

env:
  PYTHONUNBUFFERED: "1"
  RAW_PREP_OUT: data/movement_training_data.csv
  HCBC_OUT: data/multiH_binary.csv
  MODEL_BUNDLE: Resources/xgb_hcbc.bundle.joblib

jobs:
  train-and-publish:
    runs-on: ubuntu-latest
    timeout-minutes: 360

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --no-cache-dir -r "Grond-main 2/requirements.txt"
          # ensure pins for trainer
          pip install "xgboost>=1.7,<2.0" "scikit-learn>=1.3,<1.6" "optuna>=3.5" "fastapi" "pydantic" "uvicorn[standard]"

      # ======== 1) Your existing prep step (unchanged) ========
      - name: Prepare (existing pipeline)
        working-directory: "Grond-main 2"
        env:
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
        run: |
          mkdir -p data Resources
          # call your current prep the same way you do today (adjust if your command differs)
          python prepare_training_data.py --out "${{ env.RAW_PREP_OUT }}"

      - name: Verify existing prepared dataset
        run: |
          test -s "${{ env.RAW_PREP_OUT }}"
          python - << 'PY'
          import pandas as pd; import os
          p=os.environ["RAW_PREP_OUT"]; df=pd.read_csv(p)
          print("Prepared rows:", len(df)); print(df.columns.tolist()[:40])
          PY

      # ======== 2) Convert to HCBC schema ========
      - name: Convert to binary HCBC dataset
        working-directory: "Grond-main 2"
        run: |
          python prepare_hcbc_dataset.py \
            --in "${{ env.RAW_PREP_OUT }}" \
            --out "${{ env.HCBC_OUT }}" \
            --h 2 \
            --drop-neutral
          test -s "${{ env.HCBC_OUT }}"

      - name: Show HCBC head
        run: |
          python - << 'PY'
          import pandas as pd; import os
          p=os.environ["HCBC_OUT"]; df=pd.read_csv(p)
          print(df.head(5).to_string())
          print("Columns:", df.columns.tolist()[:60])
          PY

      # ======== 3) Train + Optuna + calibration ========
      - name: Train HCBC (XGBoost + Optuna)
        working-directory: "."
        run: |
          mkdir -p "Grond-main 2/Resources"
          python "audit_fixes/train_hcbc_xgb_optuna.py" \
            --data "${{ env.HCBC_OUT }}" \
            --out "${{ env.MODEL_BUNDLE }}" \
            --n-trials "${{ github.event.inputs.n_trials || 40 }}" \
            --folds "${{ github.event.inputs.folds || 6 }}" \
            --embargo-rows "${{ github.event.inputs.embargo_rows || 200 }}" \
            --seed 42
          test -s "${{ env.MODEL_BUNDLE }}"

      # ======== 4) Publish artifacts ========
      - name: Upload artifacts (bundle + reports)
        uses: actions/upload-artifact@v4
        with:
          name: hcbc_model_bundle
          path: |
            ${{ env.MODEL_BUNDLE }}
            Grond-main 2/Resources/hcbc_cv_report.json
            Grond-main 2/Resources/hcbc_thresholds.json
            Grond-main 2/Resources/hcbc_features.json
          retention-days: 30

      # Optional S3 publish using OIDC role (if you already had this configured)
      - name: Configure AWS (optional)
        if: ${{ secrets.AWS_ROLE_ARN != '' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ secrets.AWS_REGION || 'us-east-1' }}

      - name: Upload to S3 (optional)
        if: ${{ secrets.S3_MODEL_URI != '' }}
        run: |
          aws s3 cp "${{ env.MODEL_BUNDLE }}" "${{ secrets.S3_MODEL_URI }}" --only-show-errors || true
          aws s3 cp "Grond-main 2/Resources/hcbc_cv_report.json"      "${{ secrets.S3_REPORTS_PREFIX }}/hcbc_cv_report.json" --only-show-errors || true
          aws s3 cp "Grond-main 2/Resources/hcbc_thresholds.json"     "${{ secrets.S3_REPORTS_PREFIX }}/hcbc_thresholds.json" --only-show-errors || true
          aws s3 cp "Grond-main 2/Resources/hcbc_features.json"       "${{ secrets.S3_REPORTS_PREFIX }}/hcbc_features.json" --only-show-errors || true
