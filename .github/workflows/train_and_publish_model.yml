name: Train & Publish HCBC Model (Polygon required, CI env, robust data discovery)

on:
  workflow_dispatch:
    inputs:
      n_trials:
        description: 'Optuna trials'
        required: false
        default: '40'
      folds:
        description: 'Walk-forward folds'
        required: false
        default: '6'
      embargo_rows:
        description: 'Embargo rows between train/val'
        required: false
        default: '200'

permissions:
  contents: write
  id-token: write

env:
  PYTHONUNBUFFERED: "1"

  ARTIFACT_DIR: artifacts
  HCBC_OUT: artifacts/multiH_binary.csv
  MODEL_BUNDLE: artifacts/xgb_hcbc.bundle.joblib

  # CI environment secrets (job uses environment: CI)
  AWS_REGION: ${{ secrets.AWS_REGION }}
  ML_MODEL_PATH: ${{ secrets.ML_MODEL_PATH }}
  POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}

jobs:
  train-and-publish:
    runs-on: ubuntu-latest
    environment: CI
    timeout-minutes: 720

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install --no-cache-dir -r "Grond-main 2/requirements.txt"
          pip install "xgboost>=1.7,<2.0" "scikit-learn>=1.3,<1.6" "optuna>=3.5" \
                      "fastapi" "pydantic" "uvicorn[standard]" "pandas" "numpy" \
                      "pyarrow" "fastparquet"

      - name: Assert POLYGON_API_KEY is present
        run: |
          set -euo pipefail
          if [ -z "${POLYGON_API_KEY:-}" ]; then
            echo "::error::POLYGON_API_KEY is not set in the CI environment."
            exit 1
          fi
          echo "POLYGON_API_KEY detected (hidden)."

      - name: Prepare directories
        run: |
          set -euo pipefail
          mkdir -p "Grond-main 2/data" "Grond-main 2/Resources" "${ARTIFACT_DIR}"

      # ===================== 1) PREP (logs captured) =====================
      - name: Run prepare_training_data.py (Polygon)
        env:
          POLYGON_API_KEY: ${{ env.POLYGON_API_KEY }}
        run: |
          set -euo pipefail
          export POLYGON_API_KEY
          python "Grond-main 2/prepare_training_data.py" 2>&1 | tee "${ARTIFACT_DIR}/prep.log"
          test ${PIPESTATUS[0]} -eq 0

      # ===================== 2) DISCOVER & NORMALIZE DATA =================
      - name: Discover dataset under Grond-main 2/data and normalize to CSV
        run: |
          set -euo pipefail
          python - << 'PY'
          import os, glob, sys
          import pandas as pd

          data_dir = os.path.join("Grond-main 2", "data")
          if not os.path.isdir(data_dir):
            print(f"::error::Data directory missing: {data_dir}", file=sys.stderr)
            sys.exit(1)

          patterns = ["*.csv", "*.parquet", "*.feather"]
          cands = []
          for pat in patterns:
            cands.extend(glob.glob(os.path.join(data_dir, pat)))

          if not cands:
            print(f"::error::No files found in {data_dir}", file=sys.stderr)
            sys.exit(1)

          newest = max(cands, key=lambda p: os.path.getmtime(p))
          root, ext = os.path.splitext(newest)
          out_csv = os.path.join("artifacts", "prepared_dataset.csv")
          os.makedirs("artifacts", exist_ok=True)

          try:
            ext = ext.lower()
            if ext == ".csv":
              df_head = pd.read_csv(newest, nrows=1)
              if df_head.empty:
                print(f"::error::CSV appears to contain only a header (no rows): {newest}", file=sys.stderr)
                sys.exit(1)
              import shutil
              shutil.copy2(newest, out_csv)

            elif ext == ".parquet":
              df = pd.read_parquet(newest)
              if df.empty:
                print(f"::error::Parquet file is empty: {newest}", file=sys.stderr)
                sys.exit(1)
              df.to_csv(out_csv, index=False)

            elif ext == ".feather":
              df = pd.read_feather(newest)
              if df.empty:
                print(f"::error::Feather file is empty: {newest}", file=sys.stderr)
                sys.exit(1)
              df.to_csv(out_csv, index=False)

            else:
              print(f"::error::Unsupported extension {ext} for {newest}", file=sys.stderr)
              sys.exit(1)

          except Exception as e:
            print(f"::error::Failed to normalize dataset {newest} to CSV: {e}", file=sys.stderr)
            sys.exit(1)

          with open(os.environ["GITHUB_ENV"], "a") as f:
            f.write(f"RAW_PREP_OUT={out_csv}\n")

          df = pd.read_csv(out_csv, nrows=200)
          print("Discovered source:", newest)
          print("Normalized CSV:", out_csv)
          print("Columns:", df.columns.tolist())
          print(df.head(5).to_string())
          PY

      # ===================== 3) ENSURE HCBC SCHEMA =======================
      - name: Ensure HCBC schema (H + label_up)
        run: |
          set -euo pipefail
          if python - << 'PY'
          import pandas as pd, os
          df = pd.read_csv(os.environ["RAW_PREP_OUT"], nrows=1)
          cols = set(map(str.lower, df.columns))
          print(not ({"h","label_up"} <= cols))
          PY
          then
            python "Grond-main 2/prepare_hcbc_dataset.py" \
              --in "${RAW_PREP_OUT}" \
              --out "${HCBC_OUT}" \
              --h 2 \
              --drop-neutral
          else
            mkdir -p "${ARTIFACT_DIR}"
            cp "${RAW_PREP_OUT}" "${HCBC_OUT}"
            echo "HCBC: source already had H & label_up; copied to ${HCBC_OUT}"
          fi
          test -s "${HCBC_OUT}"

      - name: Show HCBC head
        run: |
          set -euo pipefail
          python - << 'PY'
          import pandas as pd, os
          p=os.environ["HCBC_OUT"]; df=pd.read_csv(p, nrows=200)
          print("HCBC columns:", df.columns.tolist())
          print(df.head(5).to_string())
          PY

      # ===================== 4) TRAIN (logs captured) ====================
      - name: Train HCBC (XGBoost + Optuna)
        run: |
          set -euo pipefail
          python "audit_fixes/train_hcbc_xgb_optuna.py" \
            --data "${HCBC_OUT}" \
            --out "${MODEL_BUNDLE}" \
            --n-trials "${{ github.event.inputs.n_trials || 40 }}" \
            --folds "${{ github.event.inputs.folds || 6 }}" \
            --embargo-rows "${{ github.event.inputs.embargo_rows || 200 }}" \
            --seed 42 2>&1 | tee "${ARTIFACT_DIR}/train.log"
          test ${PIPESTATUS[0]} -eq 0
          test -s "${MODEL_BUNDLE}"

      # ===================== 5) UPLOAD ARTIFACTS ========================
      - name: Upload artifacts (bundle + reports + logs)
        uses: actions/upload-artifact@v4
        with:
          name: hcbc_model_bundle_and_logs
          path: |
            artifacts/xgb_hcbc.bundle.joblib
            artifacts/hcbc_cv_report.json
            artifacts/hcbc_thresholds.json
            artifacts/hcbc_features.json
            artifacts/prep.log
            artifacts/train.log
            artifacts/prepared_dataset.csv
          retention-days: 30

      # ===================== 6) AWS: OIDC + S3 publish ==================
      - name: Configure AWS (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::719070647059:role/tigermoon
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Upload model bundle to S3 (ML_MODEL_PATH)
        if: ${{ env.ML_MODEL_PATH }}
        run: |
          set -euo pipefail
          echo "Uploading model to ${ML_MODEL_PATH}"
          aws s3 cp "${MODEL_BUNDLE}" "${ML_MODEL_PATH}" --only-show-errors

      - name: Upload logs to S3 (co-located with model)
        if: ${{ env.ML_MODEL_PATH }}
        run: |
          set -euo pipefail
          TS=$(date -u +"%Y%m%dT%H%M%SZ")
          BUCKET="$(echo "${ML_MODEL_PATH}" | sed 's@^s3://@@' | cut -d/ -f1)"
          KEY="$(echo "${ML_MODEL_PATH}" | sed 's@^s3://@@' | cut -d/ -f2-)"
          BASE_DIR="$(dirname "${KEY}")"
          PREFIX="s3://${BUCKET}/${BASE_DIR}/logs/${TS}"
          echo "Uploading logs to ${PREFIX}"
          aws s3 cp "artifacts/prep.log"  "${PREFIX}/prep.log"  --only-show-errors || true
          aws s3 cp "artifacts/train.log" "${PREFIX}/train.log" --only-show-errors || true
          if [ -n "${RAW_PREP_OUT:-}" ] && [ -f "${RAW_PREP_OUT}" ]; then
            aws s3 cp "${RAW_PREP_OUT}" "${PREFIX}/prepared_dataset_snapshot.csv" --only-show-errors || true
          fi
