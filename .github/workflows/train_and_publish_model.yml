name: train-and-publish

on:
  workflow_dispatch:
  push:
    branches:
      - main

permissions:
  contents: read
  id-token: write

env:
  PYTHON_VERSION: "3.11"
  ML_MODEL_PATH: "Resources/xgb_hcbc.bundle.joblib"
  DATA_DIR: "data"
  MODELS_DIR: "models"
  LOGS_DIR: "logs"
  HCBC_H_KEY: "H"
  HCBC_DEFAULT_H: "15"

jobs:
  train-and-publish:
    runs-on: ubuntu-latest

    defaults:
      run:
        working-directory: "Grond-main 2"
        shell: bash

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.PYTHON_VERSION }}-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.PYTHON_VERSION }}-

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # ────────────────────────── DATA PREP ──────────────────────────
      - name: Run prepare_training_data.py (Polygon)
        env:
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
          DATA_DIR: ${{ env.DATA_DIR }}
        run: |
          set -euo pipefail
          mkdir -p "${DATA_DIR}"
          python prepare_training_data.py
          echo "✅ prepare_training_data.py completed."

      - name: Discover dataset under data and verify CSV exists
        run: |
          set -euo pipefail
          shopt -s nullglob
          files=("${DATA_DIR}"/*.csv)
          if (( ${#files[@]} == 0 )); then
            echo "::error::No CSV found in '${DATA_DIR}'. PWD=$(pwd)"
            ls -la
            exit 1
          fi
          echo "Found CSV(s): ${files[*]}"
          csv="${DATA_DIR}/movement_training_data.csv"
          if [[ ! -f "$csv" ]]; then
            echo "::error::Expected CSV at '$csv' but not found"
            exit 1
          fi
          rows=$(( $(wc -l < "$csv") - 1 ))
          cols=$(head -n1 "$csv" | awk -F',' '{print NF}')
          sha=$(sha256sum "$csv" | awk '{print $1}')
          echo "MANIFEST path=$csv rows=$rows cols=$cols sha256=${sha:0:16}"

      # ────────────────────────── HCBC DATASET ──────────────────────────
      - name: Ensure HCBC schema (adds H + label_up)
        env:
          DATA_DIR: ${{ env.DATA_DIR }}
          HCBC_H_KEY: ${{ env.HCBC_H_KEY }}
          HCBC_DEFAULT_H: ${{ env.HCBC_DEFAULT_H }}
        run: |
          set -euo pipefail
          mkdir -p "${DATA_DIR}"
          python audit_fixes/prepare_hcbc_dataset.py \
            --input "${DATA_DIR}/movement_training_data.csv" \
            --output "${DATA_DIR}/hcbc_training.csv" \
            --h-key "${HCBC_H_KEY}" \
            --default-h "${HCBC_DEFAULT_H}" \
            --drop-neutral

      - name: Show HCBC head (sanity)
        env:
          DATA_DIR: ${{ env.DATA_DIR }}
          HCBC_H_KEY: ${{ env.HCBC_H_KEY }}
        run: |
          set -euo pipefail
          python -c "import os,pandas as pd; d=os.environ.get('DATA_DIR','data'); h=os.environ.get('HCBC_H_KEY','H'); df=pd.read_csv(f'{d}/hcbc_training.csv'); print(df.head(10).to_string()); print('\nColumns:', list(df.columns)); assert 'label_up' in df.columns, 'HCBC dataset missing label_up'; assert h in df.columns, f'HCBC dataset missing horizon key {h}'"

      # ────────────────────────── HCBC TRAIN (XGB + Optuna) ──────────────────────────
      - name: Train HCBC (XGBoost + Optuna)
        env:
          DATA_DIR: ${{ env.DATA_DIR }}
          MODELS_DIR: ${{ env.MODELS_DIR }}
          LOGS_DIR: ${{ env.LOGS_DIR }}
          HCBC_H_KEY: ${{ env.HCBC_H_KEY }}
          N_TRIALS: "40"
          RANDOM_STATE: "42"
        run: |
          set -euo pipefail
          mkdir -p "${MODELS_DIR}" "${LOGS_DIR}" Resources
          python audit_fixes/train_hcbc_xgb_optuna.py \
            --train-csv "${DATA_DIR}/hcbc_training.csv" \
            --h-key "${HCBC_H_KEY}" \
            --out-bundle "${{ env.ML_MODEL_PATH }}" \
            --out-report "${LOGS_DIR}/hcbc_report.json" \
            --n-trials "${N_TRIALS}" \
            --random-state "${RANDOM_STATE}"
          echo "✅ HCBC bundle created at ${{ env.ML_MODEL_PATH }}"

      # ────────────────────────── ARTIFACTS ──────────────────────────
      - name: Upload artifacts (bundle + reports + logs)
        uses: actions/upload-artifact@v4
        with:
          name: hcbc-model-and-logs
          path: |
            Grond-main 2/${{ env.ML_MODEL_PATH }}
            Grond-main 2/${{ env.LOGS_DIR }}/
            Grond-main 2/${{ env.MODELS_DIR }}/
          if-no-files-found: error
          retention-days: 7

      # ────────────────────────── AWS PUBLISH (OIDC) ──────────────────────────
      - name: Configure AWS (OIDC)
        if: ${{ secrets.AWS_ROLE_ARN != '' && secrets.AWS_REGION != '' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Upload model bundle to S3 (ML_MODEL_PATH)
        if: ${{ secrets.S3_BUCKET != '' && secrets.AWS_ROLE_ARN != '' && secrets.AWS_REGION != '' }}
        run: |
          set -euo pipefail
          MODEL="${{ env.ML_MODEL_PATH }}"
          DEST="s3://${{ secrets.S3_BUCKET }}/models/hcbc/$(date +%Y%m%d-%H%M%S)-bundle.joblib"
          aws s3 cp "${MODEL}" "${DEST}"
          echo "s3_uri=${DEST}" >> $GITHUB_OUTPUT

      - name: Upload logs to S3 (co-located with model)
        if: ${{ secrets.S3_BUCKET != '' && secrets.AWS_ROLE_ARN != '' && secrets.AWS_REGION != '' }}
        run: |
          set -euo pipefail
          PREFIX="s3://${{ secrets.S3_BUCKET }}/models/hcbc/logs/$(date +%Y%m%d-%H%M%S)/"
          aws s3 cp "${LOGS_DIR}" "${PREFIX}" --recursive
          echo "logs_prefix=${PREFIX}" >> $GITHUB_OUTPUT
