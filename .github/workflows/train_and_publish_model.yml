name: Generate Data, Train & Publish Model

on:
  push:
    paths:
      - "Grond-main 2/**"
      - ".github/workflows/train_and_publish_model.yml"
  workflow_dispatch:

jobs:
  train-and-upload:
    name: Train & Upload to S3
    runs-on: ubuntu-latest
    environment: CI  # must match your Environment name exactly

    permissions:
      id-token: write
      contents: read

    # Expose Polygon key to later steps (prepare_training_data.py). Not used during early S3 preflight.
    env:
      POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # ── 1) OIDC to AWS (needed for S3 preflight) ─────────────────────────────
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::719070647059:role/tigermoon
          role-session-name: tinabobina-ci
          role-duration-seconds: 21600
          aws-region: eu-north-1

      # ── 2) EARLY S3 PREFLIGHT: validate ML_MODEL_PATH and write/delete access ─
      - name: Preflight model S3 URI (early)
        id: model_s3
        env:
          ML_MODEL_PATH: ${{ vars.ML_MODEL_PATH }}   # e.g., s3://tinabobina/models/xgb_classifier.pipeline.joblib
          DEFAULT_URI: s3://tinabobina/models/xgb_classifier.pipeline.joblib
        run: |
          set -euo pipefail
          URI="${ML_MODEL_PATH:-$DEFAULT_URI}"
          if [[ "${URI}" != s3://* ]]; then
            echo "ML_MODEL_PATH/DEFAULT_URI must be s3://... ; got: ${URI}" >&2
            exit 1
          fi

          bucket="${URI#s3://}"; bucket="${bucket%%/*}"
          rest="${URI#s3://$bucket/}"
          if [[ -z "${bucket}" || -z "${rest}" || "${rest}" == "${URI}" ]]; then
            echo "Malformed S3 URI (missing bucket or key): ${URI}" >&2
            exit 1
          fi
          key="${rest}"
          prefix="$(dirname "${key}")"

          # Bucket access check
          aws s3api head-bucket --bucket "$bucket" >/dev/null 2>&1 || {
            echo "Cannot access bucket: s3://${bucket}" >&2
            exit 2
          }

          # Region note (warn only)
          want_region="${AWS_REGION:-${AWS_DEFAULT_REGION:-eu-north-1}}"
          got_region="$(aws s3api get-bucket-location --bucket "$bucket" --query 'LocationConstraint' --output text)"
          [[ "${got_region}" == "None" ]] && got_region="us-east-1"
          if [[ -n "${want_region}" && "${got_region}" != "${want_region}" ]]; then
            echo "NOTE: Bucket region is ${got_region}, runner region is ${want_region}." >&2
          fi

          # Probe write/delete in the model prefix
          tmp="/tmp/preflight.txt"
          echo "ok $(date -u +%FT%TZ)" > "$tmp"
          probe_key="${prefix%/}/_preflight_${GITHUB_RUN_ID}_${GITHUB_SHA}.txt"
          aws s3 cp "$tmp" "s3://${bucket}/${probe_key}" --only-show-errors
          aws s3 rm "s3://${bucket}/${probe_key}" --only-show-errors
          echo "Preflight write/delete OK for s3://${bucket}/${prefix%/}/"

          # Export validated URI for later upload step
          echo "model_uri=s3://${bucket}/${key}" >> "$GITHUB_OUTPUT"

      # (Optional) root-level S3 sanity; useful during IAM hardening
      - name: Verify caller & root write/delete (optional)
        env:
          BUCKET: tinabobina
        run: |
          set -euo pipefail
          aws sts get-caller-identity
          echo "ok $(date -u +%FT%TZ)" > /tmp/root-preflight.txt
          KEY="ci-preflight/${GITHUB_RUN_ID}-${GITHUB_SHA}.txt"
          aws s3 cp /tmp/root-preflight.txt "s3://${BUCKET}/${KEY}" --only-show-errors
          aws s3 rm "s3://${BUCKET}/${KEY}" --only-show-errors
          echo "Root S3 preflight OK."

      # ── 3) Python 3.11 after S3 is confirmed ─────────────────────────────────
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: "${{ runner.os }}-pip-${{ hashFiles('Grond-main 2/requirements-ml.txt', 'Grond-main 2/requirements.txt') }}"
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies (prefers requirements-ml.txt)
        working-directory: "Grond-main 2"
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip setuptools wheel
          export PIP_ONLY_BINARY=:all:
          if [[ -f "requirements-ml.txt" ]]; then
            pip install --no-cache-dir -r requirements-ml.txt awscli
          else
            pip install --no-cache-dir -r requirements.txt awscli
          fi

      - name: Ensure local utils is a package
        working-directory: "Grond-main 2"
        run: |
          set -euo pipefail
          python - << 'PY'
          import pathlib
          p = pathlib.Path('utils/__init__.py')
          p.parent.mkdir(parents=True, exist_ok=True)
          if not p.exists():
              p.write_text('# package\n')
          print("utils package OK")
          PY

      # ── 4) Data prep (uses Polygon), then training ───────────────────────────
      - name: Prepare training data
        working-directory: "Grond-main 2"
        run: |
          set -euo pipefail
          mkdir -p data
          python prepare_training_data.py
          test -s data/movement_training_data.csv

      - name: Training CSV sanity report
        working-directory: "Grond-main 2"
        run: |
          set -euo pipefail
          python - << 'PY'
          import pandas as pd
          df = pd.read_csv("data/movement_training_data.csv")
          print({"rows": len(df), "cols": list(df.columns)})
          micro = ["ms_spread_mean","ms_depth_imbalance_mean","ms_ofi_sum","ms_signed_volume_sum","ms_vpin"]
          existing = [c for c in micro if c in df.columns]
          if existing:
              miss = df[existing].isna().mean().to_dict()
              print({"microstructure_missing_rate": miss})
          PY

      - name: Train model pipeline
        working-directory: "Grond-main 2"
        run: |
          set -euo pipefail
          mkdir -p models
          python train_ml_classifier.py \
            --train-csv data/movement_training_data.csv \
            --label-col movement_type \
            --model-dir models \
            --model-filename xgb_classifier.pipeline.joblib
          test -s models/xgb_classifier.pipeline.joblib

      # ── 5) Uploads ───────────────────────────────────────────────────────────
      - name: Upload training CSV to S3
        working-directory: "Grond-main 2"
        run: |
          set -euo pipefail
          aws s3 cp data/movement_training_data.csv "s3://tinabobina/data/movement_training_data.csv" --only-show-errors

      - name: Upload model artifact to S3
        working-directory: "Grond-main 2"
        run: |
          set -euo pipefail
          URI="${{ steps.model_s3.outputs.model_uri }}"
          echo "Uploading model -> ${URI}"
          aws s3 cp models/xgb_classifier.pipeline.joblib "${URI}" --only-show-errors

      - name: Upload artifacts to GitHub
        uses: actions/upload-artifact@v4
        with:
          name: model-and-data
          path: |
            Grond-main 2/data/movement_training_data.csv
            Grond-main 2/models/xgb_classifier.pipeline.joblib
          if-no-files-found: error
