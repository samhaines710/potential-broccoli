name: Train & Publish HCBC Model (Polygon required, CI env, auto-discover prep CSV)

on:
  workflow_dispatch:
    inputs:
      n_trials:
        description: 'Optuna trials'
        required: false
        default: '40'
      folds:
        description: 'Walk-forward folds'
        required: false
        default: '6'
      embargo_rows:
        description: 'Embargo rows between train/val'
        required: false
        default: '200'

permissions:
  contents: write
  id-token: write

env:
  PYTHONUNBUFFERED: "1"

  # Downstream artifact paths
  HCBC_OUT: artifacts/multiH_binary.csv
  MODEL_BUNDLE: artifacts/xgb_hcbc.bundle.joblib

  # CI environment-scoped secrets
  AWS_REGION: ${{ secrets.AWS_REGION }}
  ML_MODEL_PATH: ${{ secrets.ML_MODEL_PATH }}
  POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}

jobs:
  train-and-publish:
    runs-on: ubuntu-latest
    environment: CI
    timeout-minutes: 360

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install --no-cache-dir -r "Grond-main 2/requirements.txt"
          pip install "xgboost>=1.7,<2.0" "scikit-learn>=1.3,<1.6" "optuna>=3.5" \
                      "fastapi" "pydantic" "uvicorn[standard]" "pandas" "numpy"

      # ----- Hard fail if Polygon key missing -----
      - name: Assert POLYGON_API_KEY is present
        run: |
          set -euo pipefail
          if [ -z "${POLYGON_API_KEY:-}" ]; then
            echo "::error::POLYGON_API_KEY is not set in the CI environment."
            exit 1
          fi
          echo "POLYGON_API_KEY detected (hidden)."

      # ======== 1) Prepare (existing pipeline; requires Polygon) ========
      - name: Prepare dataset with Polygon
        env:
          POLYGON_API_KEY: ${{ env.POLYGON_API_KEY }}   # ensure child process sees it
        run: |
          set -euo pipefail
          mkdir -p "Grond-main 2/data" "Grond-main 2/Resources" "artifacts"
          export POLYGON_API_KEY
          python "Grond-main 2/prepare_training_data.py"

      # ======== 1b) Auto-discover the CSV your prep produced ========
      - name: Discover prepared CSV path
        id: discover_csv
        run: |
          set -euo pipefail
          echo "Searching for the newest CSV under 'Grond-main 2/'..."
          python - << 'PY'
          import os, glob, json
          root = "Grond-main 2"
          # Prefer names that look like training outputs
          preferred = [
              "*movement*training*data*.csv", "*movement*training*.csv",
              "*training*data*.csv", "*prepared*.csv", "*training*.csv", "*.csv"
          ]
          candidates = []
          for pat in preferred:
              candidates.extend(glob.glob(os.path.join(root, "**", pat), recursive=True))
              if candidates:
                  break  # stop at first pattern that yields results
          if not candidates:
              raise SystemExit("::error::No CSV files found under 'Grond-main 2/'.")
          # Pick newest by mtime
          newest = max(candidates, key=lambda p: os.path.getmtime(p))
          print(f"Discovered CSV: {newest}")
          # Export for later steps
          with open(os.environ["GITHUB_ENV"], "a") as f:
              f.write(f"RAW_PREP_OUT={newest}\n")
          PY

      - name: Verify prepared dataset (discovered path)
        run: |
          set -euo pipefail
          echo "RAW_PREP_OUT=${RAW_PREP_OUT}"
          test -s "${RAW_PREP_OUT}"
          python - << 'PY'
          import pandas as pd, os
          p=os.environ["RAW_PREP_OUT"]; df=pd.read_csv(p)
          print("Prepared rows:", len(df))
          print("Columns (first 40):", df.columns.tolist()[:40])
          print(df.head(3).to_string())
          PY

      # ======== 2) Convert to HCBC schema (binary label_up + H) ========
      - name: Convert to binary HCBC dataset
        run: |
          set -euo pipefail
          mkdir -p artifacts
          python "Grond-main 2/prepare_hcbc_dataset.py" \
            --in "${RAW_PREP_OUT}" \
            --out "${HCBC_OUT}" \
            --h 2 \
            --drop-neutral
          test -s "${HCBC_OUT}"

      - name: Show HCBC head
        run: |
          set -euo pipefail
          ls -lah artifacts || true
          python - << 'PY'
          import pandas as pd, os
          p=os.environ["HCBC_OUT"]; df=pd.read_csv(p)
          print(df.head(5).to_string())
          print("HCBC columns:", df.columns.tolist())
          PY

      # ======== 3) Train + Optuna + per-H calibration ========
      - name: Train HCBC (XGBoost + Optuna)
        run: |
          set -euo pipefail
          mkdir -p "artifacts"
          python "audit_fixes/train_hcbc_xgb_optuna.py" \
            --data "${HCBC_OUT}" \
            --out "${MODEL_BUNDLE}" \
            --n-trials "${{ github.event.inputs.n_trials || 40 }}" \
            --folds "${{ github.event.inputs.folds || 6 }}" \
            --embargo-rows "${{ github.event.inputs.embargo_rows || 200 }}" \
            --seed 42
          test -s "${MODEL_BUNDLE}"

      # ======== 4) Publish artifacts ========
      - name: Upload artifacts (bundle + reports)
        uses: actions/upload-artifact@v4
        with:
          name: hcbc_model_bundle
          path: |
            artifacts/xgb_hcbc.bundle.joblib
            artifacts/hcbc_cv_report.json
            artifacts/hcbc_thresholds.json
            artifacts/hcbc_features.json
          retention-days: 30

      # ======== 5) Configure AWS (OIDC) and upload model to S3 ========
      - name: Configure AWS (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::719070647059:role/tigermoon
          aws-region: ${{ env.AWS_REGION || 'us-east-1' }}

      - name: Upload model to S3 (ML_MODEL_PATH)
        if: ${{ env.ML_MODEL_PATH }}
        run: |
          set -euo pipefail
          echo "Uploading model to ${ML_MODEL_PATH}"
          aws s3 cp "${MODEL_BUNDLE}" "${ML_MODEL_PATH}" --only-show-errors