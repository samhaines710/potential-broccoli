name: Generate Data, Train & Publish Model

on:
  push:
    paths:
      - "Grond-main 2/**"
      - ".github/workflows/train_and_publish_model.yml"
  workflow_dispatch:

jobs:
  train-and-upload:
    name: Train & Upload to S3
    runs-on: ubuntu-latest
    environment: CI   # <- must match your Environment name exactly

    # Required for GitHub OIDC â†’ STS
    permissions:
      id-token: write
      contents: read

    # Make POLYGON_API_KEY visible to all steps and Python processes
    env:
      POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: "${{ runner.os }}-pip-${{ hashFiles('Grond-main 2/requirements.txt') }}"
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        working-directory: "Grond-main 2"
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install --no-cache-dir -r requirements.txt boto3 awscli

      - name: Ensure local utils is a package
        working-directory: "Grond-main 2"
        run: |
          set -euo pipefail
          python - << 'PY'
          import pathlib
          p = pathlib.Path('utils/__init__.py')
          p.parent.mkdir(parents=True, exist_ok=True)
          if not p.exists():
              p.write_text('# package\n')
          print("utils package OK")
          PY

      # --- OIDC: assume your role "tigermoon" ---
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::719070647059:role/tigermoon
          role-session-name: tinabobina-ci
          role-duration-seconds: 21600
          aws-region: eu-north-1

      # Fail fast so long training doesn't die at upload time
      - name: Verify caller & preflight S3 write/delete
        env:
          BUCKET: tinabobina
        run: |
          set -euo pipefail
          aws sts get-caller-identity
          echo "ok $(date -u +%FT%TZ)" > /tmp/preflight.txt
          KEY="ci-preflight/${GITHUB_RUN_ID}-${GITHUB_SHA}.txt"
          aws s3 cp /tmp/preflight.txt "s3://${BUCKET}/${KEY}" --only-show-errors
          aws s3 rm "s3://${BUCKET}/${KEY}" --only-show-errors
          echo "Preflight S3 write/delete OK."

      # Fail fast if the Polygon key isn't visible/valid
      - name: Preflight Polygon API key
        run: |
          set -euo pipefail
          if [[ -z "${POLYGON_API_KEY:-}" ]]; then
            echo "POLYGON_API_KEY not visible to this job (wrong environment or missing secret)." >&2
            exit 2
          fi
          # Tiny call just to verify credentials/entitlements are good
          code=$(curl -s -o /dev/null -w "%{http_code}" \
            "https://api.polygon.io/v2/aggs/ticker/SPY/range/1/day/2025-08-01/2025-08-02?limit=1&apiKey=${POLYGON_API_KEY}")
          if [[ "$code" != "200" ]]; then
            echo "Polygon preflight failed, HTTP $code (bad key or missing entitlements)." >&2
            exit 3
          fi
          echo "Polygon key OK."

      - name: Prepare training data
        working-directory: "Grond-main 2"
        run: |
          set -euo pipefail
          mkdir -p data
          python prepare_training_data.py
          test -s data/movement_training_data.csv

      - name: Train model pipeline
        working-directory: "Grond-main 2"
        run: |
          set -euo pipefail
          mkdir -p models
          python train_ml_classifier.py \
            --train-csv data/movement_training_data.csv \
            --label-col movement_type \
            --model-dir models \
            --model-filename xgb_classifier.pipeline.joblib
          test -s models/xgb_classifier.pipeline.joblib

      - name: Upload training CSV to S3
        working-directory: "Grond-main 2"
        run: |
          set -euo pipefail
          aws s3 cp data/movement_training_data.csv "s3://tinabobina/data/movement_training_data.csv" --only-show-errors

      - name: Upload model artifact to S3
        working-directory: "Grond-main 2"
        env:
          MODEL_URI: ${{ vars.ML_MODEL_PATH }}
        run: |
          set -euo pipefail
          if [[ -z "${MODEL_URI:-}" ]]; then
            echo "Repository variable ML_MODEL_PATH is required (e.g., s3://tinabobina/models/xgb_classifier.pipeline.joblib)" >&2
            exit 1
          fi
          if [[ "${MODEL_URI}" != s3://* ]]; then
            echo "ML_MODEL_PATH must be an s3:// URI; got: ${MODEL_URI}" >&2
            exit 1
          fi
          aws s3 cp models/xgb_classifier.pipeline.joblib "${MODEL_URI}" --only-show-errors

      - name: Upload artifacts to GitHub
        uses: actions/upload-artifact@v4
        with:
          name: model-and-data
          path: |
            Grond-main 2/data/movement_training_data.csv
            Grond-main 2/models/xgb_classifier.pipeline.joblib
          if-no-files-found: error
