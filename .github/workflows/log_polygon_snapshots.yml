name: Log Polygon Option Snapshots (CSV)

on:
  workflow_dispatch:
    inputs:
      duration_min:
        description: "Run duration in minutes"
        required: false
        default: "30"
      poll_seconds:
        description: "Poll interval (seconds)"
        required: false
        default: "15"
      near_atm_pct:
        description: "Near-ATM band as fraction (e.g. 0.05 = ±5%)"
        required: false
        default: "0.05"
      max_expiries:
        description: "Number of nearest expiries to keep"
        required: false
        default: "4"
  schedule:
    # Weekdays 14:30 UTC (09:30 ET) as a default start; adjust as you wish.
    - cron: "30 14 * * 1-5"

permissions:
  contents: read
  id-token: write

env:
  OUT_DIR: polygon_live_csv
  # read-only env mirrors of secrets for bash checks later
  POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
  AWS_REGION: ${{ secrets.AWS_REGION }}
  AWS_ROLE_ARN: ${{ secrets.AWS_ROLE_ARN }}
  S3_SNAPSHOT_PREFIX: ${{ secrets.S3_SNAPSHOT_PREFIX }}

jobs:
  log-snapshots:
    name: log-snapshots
    runs-on: ubuntu-latest
    environment: CI
    timeout-minutes: 480

    steps:
      - name: Set up job
        run: echo "Starting Polygon snapshot logger…"

      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Assert POLYGON_API_KEY present
        run: |
          set -euo pipefail
          if [ -z "${POLYGON_API_KEY:-}" ]; then
            echo "::error::POLYGON_API_KEY is not set (Actions → Secrets for CI)."
            exit 1
          fi
          echo "POLYGON_API_KEY detected."

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install runtime deps
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install --no-cache-dir requests pandas

      - name: Verify logger exists
        run: |
          set -euo pipefail
          if [ -f "scripts/log_polygon_option_snapshots_csv.py" ]; then
            echo "Logger exists at scripts/log_polygon_option_snapshots_csv.py"
          else
            echo "::error::Missing scripts/log_polygon_option_snapshots_csv.py in repo."
            exit 1
          fi

      - name: Prepare output dir
        run: |
          set -euo pipefail
          mkdir -p "${OUT_DIR}"

      - name: Run live logger loop
        env:
          INP_DURATION_MIN: ${{ github.event.inputs.duration_min }}
          INP_POLL_SECONDS: ${{ github.event.inputs.poll_seconds }}
          INP_NEAR_ATM_PCT: ${{ github.event.inputs.near_atm_pct }}
          INP_MAX_EXPIRIES: ${{ github.event.inputs.max_expiries }}
        run: |
          set -euo pipefail

          # Defaults handled in bash to avoid GH ternary expressions
          DURATION_MIN="${INP_DURATION_MIN:-}"; : "${DURATION_MIN:=30}"
          POLL_SECONDS="${INP_POLL_SECONDS:-}"; : "${POLL_SECONDS:=15}"
          NEAR_ATM_PCT="${INP_NEAR_ATM_PCT:-}"; : "${NEAR_ATM_PCT:=0.05}"
          MAX_EXPIRIES="${INP_MAX_EXPIRIES:-}"; : "${MAX_EXPIRIES:=4}"

          echo "Logger config -> duration_min=${DURATION_MIN}, poll_seconds=${POLL_SECONDS}, near_atm_pct=${NEAR_ATM_PCT}, max_expiries=${MAX_EXPIRIES}"

          END_TS=$(( $(date +%s) + DURATION_MIN*60 ))
          while [ "$(date +%s)" -lt "${END_TS}" ]; do
            python "scripts/log_polygon_option_snapshots_csv.py" \
              --out "${OUT_DIR}" \
              --poll "${POLL_SECONDS}" \
              --atm "${NEAR_ATM_PCT}" \
              --exp "${MAX_EXPIRIES}" || true
            sleep "${POLL_SECONDS}"
          done

      - name: Sanity check: sample rows & zero/NaN scan
        run: |
          set -euo pipefail
          python - << 'PY'
          import os, glob, pandas as pd, json, sys
          root=os.environ["OUT_DIR"]
          files=sorted(glob.glob(os.path.join(root,"*","*","snapshot_*.csv")))
          if not files:
            print("::warning::No CSVs produced (market closed or no near-ATM rows).")
            sys.exit(0)
          newest=files[-1]
          df=pd.read_csv(newest)
          print(f"NEWEST={newest} rows={len(df)} cols={len(df.columns)}")
          # quick quality scan
          zero_cols=[]
          for c in ["bid","ask","mid","implied_volatility","delta","gamma","theta","vega","rho","open_interest","volume"]:
            if c in df.columns and (df[c].fillna(0)==0).all():
              zero_cols.append(c)
          print("ZERO_COLUMNS:", json.dumps(zero_cols))
          print("HEAD:\n", df.head(10).to_string(index=False))
          PY

      - name: Upload CSVs as artifact (daily)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: polygon_option_snapshots_csv
          path: |
            polygon_live_csv/**/snapshot_*.csv
          retention-days: 7

      - name: Normalize optional AWS env
        run: |
          set -euo pipefail
          if [ -n "${AWS_REGION:-}" ]; then echo "AWS_REGION=${AWS_REGION}"; else echo "AWS_REGION not set; S3 upload will be skipped."; fi
          if [ -n "${AWS_ROLE_ARN:-}" ]; then echo "AWS_ROLE_ARN is set."; else echo "AWS_ROLE_ARN not set; S3 upload will be skipped."; fi
          if [ -n "${S3_SNAPSHOT_PREFIX:-}" ]; then echo "S3_SNAPSHOT_PREFIX=${S3_SNAPSHOT_PREFIX}"; else echo "S3_SNAPSHOT_PREFIX not set; S3 upload will be skipped."; fi

      - name: (Optional) Configure AWS (OIDC)
        if: ${{ env.AWS_REGION != '' && env.AWS_ROLE_ARN != '' && env.S3_SNAPSHOT_PREFIX != '' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: (Optional) Sync CSVs to S3
        if: ${{ env.AWS_REGION != '' && env.AWS_ROLE_ARN != '' && env.S3_SNAPSHOT_PREFIX != '' }}
        run: |
          set -euo pipefail
          echo "Syncing ${OUT_DIR}/ -> ${S3_SNAPSHOT_PREFIX}/"
          aws s3 sync "${OUT_DIR}/" "${S3_SNAPSHOT_PREFIX}/" --only-show-errors
          echo "Done."
