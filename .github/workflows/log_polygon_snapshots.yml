name: Log Polygon Option Snapshots (CSV)

on:
  workflow_dispatch:
    inputs:
      duration_min:
        description: "Run duration in minutes"
        required: false
        default: "390"          # 6.5 hours (RTH)
      poll_seconds:
        description: "Poll interval (seconds)"
        required: false
        default: "15"
      near_atm_pct:
        description: "Near-ATM band as fraction (e.g. 0.05 = ±5%)"
        required: false
        default: "0.05"
      max_expiries:
        description: "Number of nearest expiries to keep"
        required: false
        default: "4"
  schedule:
    # Fire at 13:30 and 14:30 UTC.
    # 13:30 UTC = 15:30 SAST (RSA) = 09:30 ET during EDT
    # 14:30 UTC = 16:30 SAST (RSA) = 09:30 ET during EST
    - cron: "30 13 * * 1-5"
    - cron: "30 14 * * 1-5"

permissions:
  contents: read
  id-token: write

env:
  OUT_DIR: polygon_live_csv
  POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
  AWS_REGION: ${{ secrets.AWS_REGION }}
  AWS_ROLE_ARN: ${{ secrets.AWS_ROLE_ARN }}
  S3_SNAPSHOT_PREFIX: ${{ secrets.S3_SNAPSHOT_PREFIX }}

jobs:
  log-snapshots:
    runs-on: ubuntu-latest
    environment: CI
    timeout-minutes: 480

    steps:
      - name: "Set up job"
        run: echo "Starting Polygon snapshot logger…"

      - name: "Checkout"
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: "Assert POLYGON_API_KEY present"
        run: |
          set -euo pipefail
          if [ -z "${POLYGON_API_KEY:-}" ]; then
            echo "::error::POLYGON_API_KEY is not set (Actions → Secrets for CI)."
            exit 1
          fi
          echo "POLYGON_API_KEY detected."

      - name: "Setup Python"
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: "Install runtime deps"
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install --no-cache-dir requests pandas holidays

      # Gate so only *real* US RTH runs proceed (handles DST + US holidays)
      - name: "Market-hours gate (US RTH 09:30–16:00 ET)"
        run: |
          set -euo pipefail
          python - << 'PY'
          from datetime import datetime, time
          import os, sys
          try:
            from zoneinfo import ZoneInfo
          except Exception:
            import pytz as _p; ZoneInfo=lambda name:_p.timezone(name)  # fallback
          try:
            from holidays import US
            us_holidays = US()
          except Exception:
            us_holidays = set()  # if pkg missing, don't block
          tz = ZoneInfo("America/New_York")
          now = datetime.now(tz)
          start, end = time(9,30), time(16,0)
          run = (now.weekday() < 5) and (now.date() not in us_holidays) and (start <= now.time() <= end)
          print(f"Now ET={now.isoformat()} run={run}")
          with open(os.environ["GITHUB_ENV"], "a") as f:
            f.write(f"RUN_RTH={'1' if run else '0'}\n")
          PY

      - name: "Resolve logger path"
        id: resolve
        if: env.RUN_RTH == '1'
        run: |
          set -euo pipefail
          CAND_ROOT="scripts/log_polygon_option_snapshots_csv.py"
          CAND_GROND="Grond-main 2/scripts/log_polygon_option_snapshots_csv.py"
          if [ -f "${CAND_ROOT}" ]; then
            LOG_SCRIPT="${CAND_ROOT}"
          elif [ -f "${CAND_GROND}" ]; then
            LOG_SCRIPT="${CAND_GROND}"
          else
            echo "::error::Missing log_polygon_option_snapshots_csv.py at scripts/ or 'Grond-main 2/scripts/'."
            exit 1
          fi
          echo "Resolved logger: ${LOG_SCRIPT}"
          echo "LOG_SCRIPT=${LOG_SCRIPT}" >> "$GITHUB_ENV"

      - name: "Prepare output dir"
        if: env.RUN_RTH == '1'
        run: |
          set -euo pipefail
          mkdir -p "${OUT_DIR}"

      - name: "Run live logger loop"
        if: env.RUN_RTH == '1'
        env:
          INP_DURATION_MIN: ${{ github.event.inputs.duration_min }}
          INP_POLL_SECONDS: ${{ github.event.inputs.poll_seconds }}
          INP_NEAR_ATM_PCT: ${{ github.event.inputs.near_atm_pct }}
          INP_MAX_EXPIRIES: ${{ github.event.inputs.max_expiries }}
        run: |
          set -euo pipefail
          DURATION_MIN="${INP_DURATION_MIN:-}"; : "${DURATION_MIN:=390}"
          POLL_SECONDS="${INP_POLL_SECONDS:-}"; : "${POLL_SECONDS:=15}"
          NEAR_ATM_PCT="${INP_NEAR_ATM_PCT:-}"; : "${NEAR_ATM_PCT:=0.05}"
          MAX_EXPIRIES="${INP_MAX_EXPIRIES:-}"; : "${MAX_EXPIRIES:=4}"
          echo "Logger config -> duration_min=${DURATION_MIN}, poll_seconds=${POLL_SECONDS}, near_atm_pct=${NEAR_ATM_PCT}, max_expiries=${MAX_EXPIRIES}"
          echo "Using script: ${LOG_SCRIPT}"
          END_TS=$(( $(date +%s) + DURATION_MIN*60 ))
          while [ "$(date +%s)" -lt "${END_TS}" ]; do
            python "${LOG_SCRIPT}" \
              --out "${OUT_DIR}" \
              --poll "${POLL_SECONDS}" \
              --atm "${NEAR_ATM_PCT}" \
              --exp "${MAX_EXPIRIES}" || true
            sleep "${POLL_SECONDS}"
          done

      - name: "Sanity check: sample rows & zero/NaN scan"
        if: env.RUN_RTH == '1'
        run: |
          set -euo pipefail
          python - << 'PY'
          import os, glob, pandas as pd, json, sys
          root=os.environ["OUT_DIR"]
          files=sorted(glob.glob(os.path.join(root,"*","*","snapshot_*.csv")))
          if not files:
            print("::warning::No CSVs produced (market closed or no near-ATM rows).")
            sys.exit(0)
          newest=files[-1]
          df=pd.read_csv(newest)
          print(f"NEWEST={newest} rows={len(df)} cols={len(df.columns)}")
          zero_cols=[]
          for c in ["bid","ask","mid","implied_volatility","delta","gamma","theta","vega","rho","open_interest","volume"]:
            if c in df.columns and (df[c].fillna(0)==0).all():
              zero_cols.append(c)
          print("ZERO_COLUMNS:", json.dumps(zero_cols))
          print("HEAD:\n" + df.head(10).to_string(index=False))
          PY

      - name: "Upload CSVs as artifact (daily)"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: polygon_option_snapshots_csv
          path: polygon_live_csv/**/snapshot_*.csv
          retention-days: 7

      - name: "Normalize optional AWS env"
        run: |
          set -euo pipefail
          if [ -n "${AWS_REGION:-}" ]; then echo "AWS_REGION=${AWS_REGION}"; else echo "AWS_REGION not set; S3 upload will be skipped."; fi
          if [ -n "${AWS_ROLE_ARN:-}" ]; then echo "AWS_ROLE_ARN is set."; else echo "AWS_ROLE_ARN not set; S3 upload will be skipped."; fi
          if [ -n "${S3_SNAPSHOT_PREFIX:-}" ]; then echo "S3_SNAPSHOT_PREFIX=${S3_SNAPSHOT_PREFIX}"; else echo "S3_SNAPSHOT_PREFIX not set; S3 upload will be skipped."; fi

      - name: "(Optional) Configure AWS (OIDC)"
        if: ${{ env.AWS_REGION != '' && env.AWS_ROLE_ARN != '' && env.S3_SNAPSHOT_PREFIX != '' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: "(Optional) Sync CSVs to S3"
        if: ${{ env.AWS_REGION != '' && env.AWS_ROLE_ARN != '' && env.S3_SNAPSHOT_PREFIX != '' }}
        run: |
          set -euo pipefail
          echo "Syncing ${OUT_DIR}/ -> ${S3_SNAPSHOT_PREFIX}/"
          aws s3 sync "${OUT_DIR}/" "${S3_SNAPSHOT_PREFIX}/" --only-show-errors
          echo "Done."